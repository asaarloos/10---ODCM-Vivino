{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team Project ODCM - Team 10\n",
        "\n",
        "## Project \n",
        "This document contains the code  to scrape data from Vivino. The extracted data could be used to analyse the influence of the price of wine on consumer ratings. The Vivino website offers over millions of different wines. The sample we are going to scrape consists of all wines originated in Spain and that are deliverable to the Netherlands. At 05-10-2024, this leaves a sample of 8,079 wines (this number could change over time). Of these wines, the following data will be extracted:\n",
        "- **Hyperlink**: The hyperlink of each wine, which includes the unique id of each wine. The unique wine id will later be isolated when cleaning the data.\n",
        "- **Brand**: The brand that produces the wine.\n",
        "- **Wine**: The name of the specific wine.\n",
        "- **Rating**: The rating of the specific wine (0-5).\n",
        "- **Price**: The price of the specific wine. When the wine is on discount, the original price will be taken, not the discounted price.\n",
        "- **Timestamp**: The timestamp at which the data is extracted. This will be useful in future analyses, so the date and time of extraction can always be found.\n",
        "\n",
        "### Code\n",
        "The code will be run individually for each type of wine: red, white, rose, sparkling, dessert and fortified. After the code of each type is run, a dataset will be created containing all wines of that type. Later, when cleaning the data, each dataset will get identified by adding an extra column with the wine category. Then, all datasets will be merged into one dataset containing all 8,057 wines. This dataset will be further cleaned and after removing duplicate wines, it will producte a final dataset with 7,585 wines.\n",
        "\n",
        "We created a makefile to automate this process. When running the makefile, it will first run this webscraping code, creating seperate datasets per type of wine. Then, the datasets will be merged into one csv file, which will then be cleaned and several summary statistics will be calculated and plots will be created. The individual files for each category created by the web scraper will be deleted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install the packages & librabies \n",
        "\n",
        "To extract the data, we make use of Beautiful Soup and Selenium. In order to run the code, first install and import the packages below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KqSSIap269zb",
        "outputId": "2e966433-ea96-4460-e83b-c65fc6cb8bf0"
      },
      "outputs": [],
      "source": [
        "#installing packages\n",
        "!pip3 install selenium\n",
        "!pip3 install webdriver_manager\n",
        "!pip3 install beautifulsoup4\n",
        "\n",
        "#libraries\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red Wines \n",
        "\n",
        "In the cell below the data of the red wines are collected. To ensure that the code doesn't break, we filter the wines to create separate extraction batches, each with their own URL. These urls are then, one by one, fed into the code using a for loop. After scraping, the data will be appended to a csv file named 'red_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "urls = ('https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1NFBLrrT181FLtnUNDVIrAEqmp9mWJRZlppYk5qjlF6XYJhYnq-UnVdoWFGUmp6qVl0THAhUlVxYD6dRiNTAJAJj7HFE%3D', # 0-10 euro\n",
        "      'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1NFDLTaywNTJQS6609fNRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5Rel2CYWJ6vlJ1XaFhRlJqeqlZdExwIVJVcWA-nUYjUwCQCq8xyD', # 10-20 euro\n",
        "      'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1MlDLTaywNTFQS6609fNRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5Rel2CYWJ6vlJ1XaFhRlJqeqlZdExwIVJVcWA-nUYjUwCQCrzByG', # 20-40 euro\n",
        "      'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1MVDLTaywNTUwUEuutPXzUUu2dQ0NUisASqen2ZYlFmWmliTmqOUXpdgmFier5SdV2hYUZSanqpWXRMcCFSVXFgPp1GI1MAkAvwMcuQ%3D%3D') # 40-500 euro\n",
        "\n",
        "# Open a csv file to store the data in\n",
        "with open('../../data/red_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "      writer = csv.writer(file)\n",
        "\n",
        "      # Write the header row\n",
        "      writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "for url in urls:\n",
        "      driver.get(url)\n",
        "      driver.maximize_window()\n",
        "\n",
        "      # Optional: Adding some wait time for the page to fully load if needed\n",
        "      driver.implicitly_wait(20)\n",
        "\n",
        "      # Create a function to click away the cookies\n",
        "      try:\n",
        "            accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "            accept_cookies_button.click()\n",
        "            print(\"Cookies accepted.\")\n",
        "      except Exception as e:\n",
        "            print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "      # Infinite scroll to load more content\n",
        "      scroll_pause_time = 2 # Adjust if necessary\n",
        "      last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "      while True:\n",
        "            # Scroll down to the bottom of the page\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "            # Wait for the new page to load\n",
        "            time.sleep(scroll_pause_time)\n",
        "    \n",
        "            # Calculate new scroll height and compare with the last scroll height\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                  time.sleep(scroll_pause_time) # Wait for the potentially new content to load\n",
        "                  new_height = driver.execute_script(\"return document.body.scrollHeight\") # Try scroling once more\n",
        "                  if new_height == last_height:\n",
        "                        break  # Stop if we've reached the end of the page\n",
        "            last_height = new_height\n",
        "\n",
        "      # Get the final page source after all content is loaded\n",
        "      page_source = driver.page_source\n",
        "\n",
        "      # Parse the page source with BeautifulSoup\n",
        "      soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "      # Create empty lists to store the data\n",
        "      hyperlink = []\n",
        "      brands = []\n",
        "      wines = []\n",
        "      ratings = []\n",
        "      reviews = []\n",
        "      prices = []\n",
        "      timestamp = []\n",
        "\n",
        "      # Find all wine entries on the page\n",
        "      wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "      for entry in wine_entries:\n",
        "\n",
        "            # Extract hyperlink\n",
        "            link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "            if link_tag and link_tag.has_attr('href'):\n",
        "                  hyperlink.append(link_tag['href'])\n",
        "\n",
        "            # Extract brand\n",
        "            brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "            if brand:\n",
        "                  brands.append(brand.get_text(strip=True))\n",
        "\n",
        "            # Extract wine name\n",
        "            wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "            if wine:\n",
        "                  wines.append(wine.get_text(strip=True))\n",
        "\n",
        "            # Extract rating\n",
        "            rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "            if rating:\n",
        "                  ratings.append(rating.get_text(strip=True))\n",
        "            # Extract review count\n",
        "            review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "            if review:\n",
        "                  # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "                  review_text = review.get_text(strip=True)\n",
        "                  review_count = review_text.split()[0]\n",
        "                  reviews.append(review_count)\n",
        "            \n",
        "            # Check for the presence of the discount first\n",
        "            discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "            if discount_price_div:\n",
        "                  # If discount exist, get the original price\n",
        "                  discount_price_text = discount_price_div.get_text(strip=True)\n",
        "                  price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                  prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "            else:  \n",
        "                  # Extract currency & price if present in the addToCartButton\n",
        "                  price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "                  if price_divs:  # If primary price class exists\n",
        "                        for price_div in price_divs:\n",
        "                              currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                              price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                              full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                              prices.append(full_price)  # Save the full price\n",
        "                  else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "                        alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "                        if alt_price_div:\n",
        "                              alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                              price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                              prices.append(price_only)  # Append only the price to the list\n",
        "            \n",
        "            # Extract timestamp\n",
        "            timestamps = time.time()\n",
        "            timestamp.append(timestamps)\n",
        "\n",
        "            # Wait for 2 seconds to not overload the server\n",
        "            time.sleep(2)\n",
        "\n",
        "      # Open a csv file to store the data in\n",
        "      with open('../../data/red_wine.csv', mode='a', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "\n",
        "            # Write the data rows\n",
        "            for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink,brands, wines, ratings, reviews, prices, timestamp):\n",
        "                  writer.writerow([\n",
        "                        hyperlink,\n",
        "                        brand,\n",
        "                        wine,\n",
        "                        rating,\n",
        "                        reviews,\n",
        "                        price,\n",
        "                        timestamp \n",
        "                  ])\n",
        "      \n",
        "      print(\"Data saved to 'red_wine.csv' successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## White Wines\n",
        "\n",
        "In the cell below the data of the white wines are collected. To ensure that the code doesn't break, we filter the wines to create separate extraction batches, each with their own URL. These urls are then, one by one, fed into the code using a for loop. After scraping, the data will be appended to a csv file named 'white_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTEMkIL-ymZW",
        "outputId": "6ca6a896-c27e-455a-f109-85dda755bc68"
      },
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "urls = ('https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1MlBLrrT181FLtnUNDVIrAEqmp9mWJRZlppYk5qjlF6XYJhYnq-UnVdoWFGUmp6qVl0TH2hoBNRUD6dRiNTAJAJlRHFM%3D', # 0-20 euro\n",
        "        'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1MlDLTaywNTUwUEuutPXzUUu2dQ0NUisASqen2ZYlFmWmliTmqOUXpdgmFier5SdV2hYUZSanqpWXRMfaGgE1FQPp1GI1MAkAvnccuA%3D%3D') # 20-500 euro\n",
        "\n",
        "# Open a csv file to store the data in\n",
        "with open('../../data/white_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "for url in urls:\n",
        "    driver.get(url)\n",
        "    driver.maximize_window()\n",
        "\n",
        "    # Optional: Adding some wait time for the page to fully load if needed\n",
        "    driver.implicitly_wait(20)\n",
        "\n",
        "    # Create a function to click away the cookies\n",
        "    try:\n",
        "        accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "        accept_cookies_button.click()\n",
        "        print(\"Cookies accepted.\")\n",
        "    except Exception as e:\n",
        "        print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "    # Infinite scroll to load more content\n",
        "    scroll_pause_time = 2 # Adjust if necessary\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    while True:\n",
        "        # Scroll down to the bottom of the page\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "        # Wait for the new page to load\n",
        "        time.sleep(scroll_pause_time)\n",
        "    \n",
        "        # Calculate new scroll height and compare with the last scroll height\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            time.sleep(scroll_pause_time) # Wait for the potentially new content to load\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\") # Try scroling once more\n",
        "            if new_height == last_height:\n",
        "                break  # Stop if we've reached the end of the page\n",
        "        last_height = new_height\n",
        "\n",
        "    # Get the final page source after all content is loaded\n",
        "    page_source = driver.page_source\n",
        "\n",
        "    # Parse the page source with BeautifulSoup\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "    # Create empty lists to store the data\n",
        "    hyperlink = []\n",
        "    brands = []\n",
        "    wines = []\n",
        "    ratings = []\n",
        "    reviews = []\n",
        "    prices = []\n",
        "    timestamp = []\n",
        "\n",
        "    # Find all wine entries on the page\n",
        "    wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "    for entry in wine_entries:\n",
        "\n",
        "        # Extract hyperlink\n",
        "        link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "        if link_tag and link_tag.has_attr('href'):\n",
        "            hyperlink.append(link_tag['href'])\n",
        "\n",
        "        # Extract brand\n",
        "        brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "        if brand:\n",
        "            brands.append(brand.get_text(strip=True))\n",
        "\n",
        "        # Extract wine name\n",
        "        wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "        if wine:\n",
        "            wines.append(wine.get_text(strip=True))\n",
        "\n",
        "        # Extract rating\n",
        "        rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "        if rating:\n",
        "            ratings.append(rating.get_text(strip=True))\n",
        "        # Extract review count\n",
        "        review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "        if review:\n",
        "            # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "            review_text = review.get_text(strip=True)\n",
        "            review_count = review_text.split()[0]\n",
        "            reviews.append(review_count)\n",
        "            \n",
        "        # Check for the presence of the discount first\n",
        "        discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "        if discount_price_div:\n",
        "            # If discount exist, get the original price\n",
        "            discount_price_text = discount_price_div.get_text(strip=True)\n",
        "            price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "            prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "        else:  \n",
        "            # Extract currency & price if present in the addToCartButton\n",
        "            price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "            if price_divs:  # If primary price class exists\n",
        "                for price_div in price_divs:\n",
        "                    currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                    price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                    full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                    prices.append(full_price)  # Save the full price\n",
        "            else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "                alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "                if alt_price_div:\n",
        "                    alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                    price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                    prices.append(price_only)  # Append only the price to the list\n",
        "            \n",
        "        # Extract timestamp\n",
        "        timestamps = time.time()\n",
        "        timestamp.append(timestamps)\n",
        "\n",
        "        # Wait for 2 seconds to not overload the server\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Open a csv file to store the data in\n",
        "    with open('../../data/white_wine.csv', mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the data rows\n",
        "        for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink,brands, wines, ratings, reviews, prices, timestamp):\n",
        "            writer.writerow([\n",
        "                hyperlink,\n",
        "                brand,\n",
        "                wine,\n",
        "                rating,\n",
        "                reviews,\n",
        "                price,\n",
        "                timestamp \n",
        "            ])\n",
        "      \n",
        "    print(\"Data saved to 'white_wine.csv' successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rose Wines\n",
        "\n",
        "In the cell below we are collecting the data of the rose wines. Since the number of rose wines is limited, it is not necessary to seperate the wines into seperate urls. Therefore, the code is only run once to extract the full set of rose wines. After scraping, the data will be appended to a csv file named 'rose_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RadMpxRG1JFH",
        "outputId": "2ccca26b-1fab-4823-a0a4-4d2396e5876d"
      },
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "# Opening the 'Vivino' website\n",
        "url = 'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1NTBQS6609fNRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5Rel2CYWJ6vlJ1XaFhRlJqeqlZdEx9qaADUVA-nUYjUwCQCsZhyI'\n",
        "driver.get(url)\n",
        "driver.maximize_window()\n",
        "\n",
        "# Optional: Adding some wait time for the page to fully load if needed\n",
        "driver.implicitly_wait(20)  # 20 seconds \n",
        "\n",
        " # Create a function to click away the cookies\n",
        "try:\n",
        "    accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "    accept_cookies_button.click()\n",
        "    print(\"Cookies accepted.\")\n",
        "except Exception as e:\n",
        "    print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "# Infinite scroll to load more content\n",
        "scroll_pause_time = 2  # Adjust if necessary\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll down to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "    # Wait for the new page to load\n",
        "    time.sleep(scroll_pause_time)\n",
        "    \n",
        "    # Calculate new scroll height and compare with the last scroll height\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        time.sleep(scroll_pause_time)  # Wait for the potentially new content to load\n",
        "        break  # Stop if we've reached the end of the page\n",
        "    last_height = new_height\n",
        "\n",
        "# Get the final page source after all content is loaded\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the page source with BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Create empty lists to store the data\n",
        "hyperlink = []\n",
        "brands = []\n",
        "wines = []\n",
        "ratings = []\n",
        "reviews = []\n",
        "prices = []\n",
        "timestamp = []\n",
        "\n",
        "# Find all wine entries on the page\n",
        "wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "for entry in wine_entries:\n",
        "\n",
        "    link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "    if link_tag and link_tag.has_attr('href'):\n",
        "        hyperlink.append(link_tag['href'])\n",
        "\n",
        "    # Extract brand\n",
        "    brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "    if brand:\n",
        "        brands.append(brand.get_text(strip=True))\n",
        "\n",
        "    # Extract wine name\n",
        "    wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "    if wine:\n",
        "        wines.append(wine.get_text(strip=True))\n",
        "\n",
        "    # Extract rating\n",
        "    rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "    if rating:\n",
        "        ratings.append(rating.get_text(strip=True))\n",
        "    \n",
        "    # Extract review count\n",
        "    review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "    if review:\n",
        "       # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "        review_text = review.get_text(strip=True)\n",
        "        review_count = review_text.split()[0]\n",
        "        reviews.append(review_count)\n",
        "\n",
        "    # Check for the presence of the discount first\n",
        "    discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "    if discount_price_div:\n",
        "        # If discount exist, get the original price\n",
        "        discount_price_text = discount_price_div.get_text(strip=True)\n",
        "        price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "        prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "    else:  \n",
        "        # Extract currency & price if present in the addToCartButton\n",
        "        price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "        if price_divs:  # If primary price class exists\n",
        "            for price_div in price_divs:\n",
        "                currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                prices.append(full_price)  # Save the full price\n",
        "        else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "            alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "            if alt_price_div:\n",
        "                alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                prices.append(price_only)  # Append only the price to the list\n",
        "                \n",
        "    timestamps = time.time()\n",
        "    timestamp.append(timestamps)\n",
        "\n",
        "    # Wait for 2 seconds to not overload the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Open csv file to store the data\n",
        "with open('../../data/rose_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink, brands, wines, ratings, reviews, prices, timestamp):\n",
        "        writer.writerow([\n",
        "            hyperlink,\n",
        "            brand,\n",
        "            wine,\n",
        "            rating,\n",
        "            reviews,\n",
        "            price,\n",
        "            timestamp\n",
        "        ])\n",
        "\n",
        "print(\"Data saved to 'rose_wine.csv' successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparkling Wines\n",
        "\n",
        "In the cell below we are collecting the data of the sparkling wines. Since the number of sparkling wines is limited, it is not necessary to seperate the wines into seperate urls. Therefore, the code is only run once with the full set of sparkling wines. After scraping, the data will be appended to a csv file named 'sparkling_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UINhqcAS1xOF",
        "outputId": "4faa9a9e-cf58-4f06-9c22-782b953c848b"
      },
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "# Opening the 'Vivino' website\n",
        "url = 'https://www.vivino.com/explore?e=eJwdi70KgCAABt_mm41o_Ma2aAiaIsLMQkgNtb-3T1ruhuNsYAFrHAWsfFgJAfWybaBY9x2OXLeVlwxGJ7nDh4WLjgp-fhlkMm6Lk_KnS7jTMLLMd8zWET8_0c4gXA%3D%3D'\n",
        "driver.get(url)\n",
        "driver.maximize_window()\n",
        "\n",
        "# Optional: Adding some wait time for the page to fully load if needed\n",
        "driver.implicitly_wait(20)  # 20 seconds \n",
        "\n",
        " # Create a function to click away the cookies\n",
        "try:\n",
        "    accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "    accept_cookies_button.click()\n",
        "    print(\"Cookies accepted.\")\n",
        "except Exception as e:\n",
        "    print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "# Infinite scroll to load more content\n",
        "scroll_pause_time = 2  # Adjust if necessary\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll down to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "    # Wait for the new page to load\n",
        "    time.sleep(scroll_pause_time)\n",
        "    \n",
        "    # Calculate new scroll height and compare with the last scroll height\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        time.sleep(scroll_pause_time)  # Wait for the potentially new content to load\n",
        "        break  # Stop if we've reached the end of the page\n",
        "    last_height = new_height\n",
        "\n",
        "# Get the final page source after all content is loaded\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the page source with BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Create empty lists to store the data\n",
        "hyperlink = []\n",
        "brands = []\n",
        "wines = []\n",
        "ratings = []\n",
        "reviews = []\n",
        "prices = []\n",
        "timestamp = []\n",
        "\n",
        "# Find all wine entries on the page\n",
        "wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "for entry in wine_entries:\n",
        "\n",
        "    link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "    if link_tag and link_tag.has_attr('href'):\n",
        "        hyperlink.append(link_tag['href'])\n",
        "\n",
        "    # Extract brand\n",
        "    brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "    if brand:\n",
        "        brands.append(brand.get_text(strip=True))\n",
        "\n",
        "    # Extract wine name\n",
        "    wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "    if wine:\n",
        "        wines.append(wine.get_text(strip=True))\n",
        "\n",
        "    # Extract rating\n",
        "    rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "    if rating:\n",
        "        ratings.append(rating.get_text(strip=True))\n",
        "    \n",
        "    # Extract review count\n",
        "    review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "    if review:\n",
        "       # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "        review_text = review.get_text(strip=True)\n",
        "        review_count = review_text.split()[0]\n",
        "        reviews.append(review_count)\n",
        "\n",
        "    # Check for the presence of the discount first\n",
        "    discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "    if discount_price_div:\n",
        "        # If discount exist, get the original price\n",
        "        discount_price_text = discount_price_div.get_text(strip=True)\n",
        "        price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "        prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "    else:  \n",
        "        # Extract currency & price if present in the addToCartButton\n",
        "        price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "        if price_divs:  # If primary price class exists\n",
        "            for price_div in price_divs:\n",
        "                currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                prices.append(full_price)  # Save the full price\n",
        "        else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "            alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "            if alt_price_div:\n",
        "                alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                prices.append(price_only)  # Append only the price to the list\n",
        "                \n",
        "    timestamps = time.time()\n",
        "    timestamp.append(timestamps)\n",
        "\n",
        "    # Wait for 2 seconds to not overload the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Open csv file to store the data\n",
        "with open('../../data/sparkling_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink, brands, wines, ratings, reviews, prices, timestamp):\n",
        "        writer.writerow([\n",
        "            hyperlink,\n",
        "            brand,\n",
        "            wine,\n",
        "            rating,\n",
        "            reviews,\n",
        "            price,\n",
        "            timestamp\n",
        "        ])\n",
        "\n",
        "print(\"Data saved to 'sparkling_wine.csv' successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dessert Wines\n",
        "\n",
        "In the cell below we are collecting the data of the dessert wines. Since the number of dessert wines is limited, it is not necessary to seperate the wines into seperate urls. Therefore, the code is only run once with the full set of dessert wines. After scraping, the data will be appended to a csv file named 'dessert_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "# Opening the 'Vivino' website\n",
        "url = 'https://www.vivino.com/explore?e=eJwdi70KgCAABt_mm22Ipm9si4agKSLMLITUUPt7-6TlbjjOBhawxlHAyoelEFAv2waKdd_hyHVbeclgdJI7fFi46Kjg55dBJuO2OCl_uoQ7DSOrfMdsHfHzA9IOIGA%3D'\n",
        "driver.get(url)\n",
        "driver.maximize_window()\n",
        "\n",
        "# Optional: Adding some wait time for the page to fully load if needed\n",
        "driver.implicitly_wait(20)  # 20 seconds \n",
        "\n",
        " # Create a function to click away the cookies\n",
        "try:\n",
        "    accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "    accept_cookies_button.click()\n",
        "    print(\"Cookies accepted.\")\n",
        "except Exception as e:\n",
        "    print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "# Infinite scroll to load more content\n",
        "scroll_pause_time = 2  # Adjust if necessary\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll down to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "    # Wait for the new page to load\n",
        "    time.sleep(scroll_pause_time)\n",
        "    \n",
        "    # Calculate new scroll height and compare with the last scroll height\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        time.sleep(scroll_pause_time)  # Wait for the potentially new content to load\n",
        "        break  # Stop if we've reached the end of the page\n",
        "    last_height = new_height\n",
        "\n",
        "# Get the final page source after all content is loaded\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the page source with BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Create empty lists to store the data\n",
        "hyperlink = []\n",
        "brands = []\n",
        "wines = []\n",
        "ratings = []\n",
        "reviews = []\n",
        "prices = []\n",
        "timestamp = []\n",
        "\n",
        "# Find all wine entries on the page\n",
        "wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "for entry in wine_entries:\n",
        "\n",
        "    link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "    if link_tag and link_tag.has_attr('href'):\n",
        "        hyperlink.append(link_tag['href'])\n",
        "\n",
        "    # Extract brand\n",
        "    brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "    if brand:\n",
        "        brands.append(brand.get_text(strip=True))\n",
        "\n",
        "    # Extract wine name\n",
        "    wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "    if wine:\n",
        "        wines.append(wine.get_text(strip=True))\n",
        "\n",
        "    # Extract rating\n",
        "    rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "    if rating:\n",
        "        ratings.append(rating.get_text(strip=True))\n",
        "    \n",
        "    # Extract review count\n",
        "    review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "    if review:\n",
        "       # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "        review_text = review.get_text(strip=True)\n",
        "        review_count = review_text.split()[0]\n",
        "        reviews.append(review_count)\n",
        "\n",
        "    # Check for the presence of the discount first\n",
        "    discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "    if discount_price_div:\n",
        "        # If discount exist, get the original price\n",
        "        discount_price_text = discount_price_div.get_text(strip=True)\n",
        "        price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "        prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "    else:  \n",
        "        # Extract currency & price if present in the addToCartButton\n",
        "        price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "        if price_divs:  # If primary price class exists\n",
        "            for price_div in price_divs:\n",
        "                currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                prices.append(full_price)  # Save the full price\n",
        "        else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "            alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "            if alt_price_div:\n",
        "                alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                prices.append(price_only)  # Append only the price to the list\n",
        "                \n",
        "    timestamps = time.time()\n",
        "    timestamp.append(timestamps)\n",
        "\n",
        "    # Wait for 2 seconds to not overload the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Open csv file to store the data\n",
        "with open('../../data/dessert_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink, brands, wines, ratings, reviews, prices, timestamp):\n",
        "        writer.writerow([\n",
        "            hyperlink,\n",
        "            brand,\n",
        "            wine,\n",
        "            rating,\n",
        "            reviews,\n",
        "            price,\n",
        "            timestamp\n",
        "        ])\n",
        "\n",
        "print(\"Data saved to 'dessert_wine.csv' successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fortified Wines\n",
        "\n",
        "In the cell below we are collecting the data of the fortified wines. Since the number of fortified wines is limited, it is not necessary to seperate the wines into seperate urls. Therefore, the code is only run once with the full set of fortified wines. After scraping, the data will be appended to a csv file named 'fortified_wine.csv' in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service)\n",
        "\n",
        "# Opening the 'Vivino' website\n",
        "url = 'https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1NTBQS6609fNRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5Rel2CYWJ6vlJ1XaFhRlJqeqlZdEx9oamQB1FQMZqcVqYBIAxtccug%3D%3D'\n",
        "driver.get(url)\n",
        "driver.maximize_window()\n",
        "\n",
        "# Optional: Adding some wait time for the page to fully load if needed\n",
        "driver.implicitly_wait(20)  # 20 seconds \n",
        "\n",
        " # Create a function to click away the cookies\n",
        "try:\n",
        "    accept_cookies_button = driver.find_element(By.ID,\"didomi-notice-agree-button\")\n",
        "    accept_cookies_button.click()\n",
        "    print(\"Cookies accepted.\")\n",
        "except Exception as e:\n",
        "    print(\"Cookie acceptance button not found or could not be clicked:\")\n",
        "\n",
        "# Infinite scroll to load more content\n",
        "scroll_pause_time = 2  # Adjust if necessary\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    # Scroll down to the bottom of the page\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    \n",
        "    # Wait for the new page to load\n",
        "    time.sleep(scroll_pause_time)\n",
        "    \n",
        "    # Calculate new scroll height and compare with the last scroll height\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        time.sleep(scroll_pause_time)  # Wait for the potentially new content to load\n",
        "        break  # Stop if we've reached the end of the page\n",
        "    last_height = new_height\n",
        "\n",
        "# Get the final page source after all content is loaded\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the page source with BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Create empty lists to store the data\n",
        "hyperlink = []\n",
        "brands = []\n",
        "wines = []\n",
        "ratings = []\n",
        "reviews = []\n",
        "prices = []\n",
        "timestamp = []\n",
        "\n",
        "# Find all wine entries on the page\n",
        "wine_entries = soup.find_all(class_='card__card--2R5Wh wineCard__wineCardContent--3cwZt')\n",
        "\n",
        "for entry in wine_entries:\n",
        "\n",
        "    link_tag = entry.find('a', class_='wineCard__cardLink--3F_uB')\n",
        "    if link_tag and link_tag.has_attr('href'):\n",
        "        hyperlink.append(link_tag['href'])\n",
        "\n",
        "    # Extract brand\n",
        "    brand = entry.find(class_='wineInfoVintage__truncate--3QAtw')\n",
        "    if brand:\n",
        "        brands.append(brand.get_text(strip=True))\n",
        "\n",
        "    # Extract wine name\n",
        "    wine = entry.find(class_='wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw') \n",
        "    if wine:\n",
        "        wines.append(wine.get_text(strip=True))\n",
        "\n",
        "    # Extract rating\n",
        "    rating = entry.find(class_='vivinoRating_averageValue__uDdPM')\n",
        "    if rating:\n",
        "        ratings.append(rating.get_text(strip=True))\n",
        "    \n",
        "    # Extract review count\n",
        "    review = entry.find(class_='vivinoRating_caption__xL84P')\n",
        "    if review:\n",
        "       # Get only the first part so the word 'beoordelingen' is not scraped\n",
        "        review_text = review.get_text(strip=True)\n",
        "        review_count = review_text.split()[0]\n",
        "        reviews.append(review_count)\n",
        "\n",
        "    # Check for the presence of the discount first\n",
        "    discount_price_div = entry.find(class_='price_strike__mOVjZ addToCart__subText--1pvFt')\n",
        "    if discount_price_div:\n",
        "        # If discount exist, get the original price\n",
        "        discount_price_text = discount_price_div.get_text(strip=True)\n",
        "        price_only = discount_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "        prices.append(price_only)  # Append only the discount price to the list\n",
        "    \n",
        "    else:  \n",
        "        # Extract currency & price if present in the addToCartButton\n",
        "        price_divs = entry.find_all(class_='addToCartButton__price--qJdh4')\n",
        "        if price_divs:  # If primary price class exists\n",
        "            for price_div in price_divs:\n",
        "                currency = price_div.find('div', class_='addToCartButton__currency--2CTNX')\n",
        "                price = price_div.find_all('div')[1]  # Assuming price is in the second div\n",
        "                full_price = f\"{currency.get_text(strip=True) if currency else ''}{price.get_text(strip=True) if price else ''}\"\n",
        "                prices.append(full_price)  # Save the full price\n",
        "        else:  # If not present, extract price from alternative class (online verkrijgbaar vanaf...)\n",
        "            alt_price_div = entry.find(class_='addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')\n",
        "            if alt_price_div:\n",
        "                alt_price_text = alt_price_div.get_text(strip=True)\n",
        "                price_only = alt_price_text.split()[-1]  # Get the last part (currency + price)\n",
        "                prices.append(price_only)  # Append only the price to the list\n",
        "                \n",
        "    timestamps = time.time()\n",
        "    timestamp.append(timestamps)\n",
        "    \n",
        "    # Wait for 2 seconds to not overload the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Open csv file to store the data\n",
        "with open('../../data/fortified_wine.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow(['hyperlink','Brand', 'Wine', 'Rating', 'Reviews', 'Price','Timestamp'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for hyperlink,brand, wine, rating, reviews, price, timestamp in zip(hyperlink, brands, wines, ratings, reviews, prices, timestamp):\n",
        "        writer.writerow([\n",
        "            hyperlink,\n",
        "            brand,\n",
        "            wine,\n",
        "            rating,\n",
        "            reviews,\n",
        "            price,\n",
        "            timestamp\n",
        "        ])\n",
        "\n",
        "print(\"Data saved to 'fortified_wine.csv' successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
